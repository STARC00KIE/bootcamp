{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수정 포인트\n",
    "1. 데이터 증강 추가, 기존에는 정규화만 있었음, resnet18 백본 사용\n",
    "2. Early Stopping, resnet18 백본 사용\n",
    "3. ConvNeXt-Tiny로 백본 변경\n",
    "4. AMP 적용, 크로스 엔트로피 로스에 라벨 스무싱 0.1 적용, 이미지 사이즈 384 적용용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'IMG_SIZE': 224, # 모델에 입력되는 이미지 크기\n",
    "# 'BATCH_SIZE': 64, # 한 번에 모델에 넣는 이미지 수\n",
    "# 'EPOCHS': 50, # 전체 데이터를 몇 번 반복 학습할지\n",
    "#'LEARNING_RATE': 가중치 업데이트 정도를 조절하는 학습률\n",
    "# 'SEED' : 무작위 요소들을 고정하여 실험 결과를 재현 가능하게 함\n",
    "# \n",
    "CFG = {\n",
    "    'IMG_SIZE': 384,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'EPOCHS': 50,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 학습 과정에서는 무작위성이 많이 개입된\n",
    "# 무작위성: 데이터 로딩 순서, 가중치 초기화, dropout, 데이터 증강, gpu 연산의 비결정성\n",
    "# 그레서 시드를 고정하지 안으면 매번 실행할 때마다 결과가 달라짐\n",
    "# 동일한 결과가 재현되도록 하기 위한 설정이 seed_everyting 함수수\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            # 테스트셋: 라벨 없이 이미지 경로만 저장\n",
    "            for fname in sorted(os.listdir(root_dir)):\n",
    "                if fname.lower().endswith(('.jpg')):\n",
    "                    img_path = os.path.join(root_dir, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            # 학습셋: 클래스별 폴더 구조에서 라벨 추출\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(root_dir, cls_name)\n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith(('.jpg')):\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = './data/train'\n",
    "test_root = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\albumentations\\core\\validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\os415\\AppData\\Local\\Temp\\ipykernel_5592\\2295189183.py:28: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=2,          # 최대 2개 영역 제거\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터에 적용할 데이터 증강 파이프라인\n",
    "train_transform = A.Compose([\n",
    "    # 이미지 크기를 모델 입력 크기에 맞게 고정\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "\n",
    "    # 좌우 반전: 차량이 좌우 어느 방향을 향해 있어도 학습 가능하도록\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "\n",
    "    # 밝기/대비 랜덤 조절: 야외 촬영 시 조명 차이를 반영\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "\n",
    "    # 색조(Hue), 채도(Saturation), 명도(Value) 변경: 색상에 덜 민감하게\n",
    "    A.HueSaturationValue(p=0.2),\n",
    "\n",
    "    # 이미지 이동, 확대/축소, 회전: 다양한 촬영 각도 및 위치 대응\n",
    "    A.ShiftScaleRotate(shift_limit=0.05,   # 최대 ±5% 이동\n",
    "                       scale_limit=0.05,   # 최대 ±5% 확대/축소\n",
    "                       rotate_limit=15,    # 최대 ±15도 회전\n",
    "                       p=0.5),\n",
    "\n",
    "    # 그림자 효과 추가: 자연광 환경에서의 촬영 상황을 반영\n",
    "    A.RandomShadow(p=0.2),\n",
    "\n",
    "    # RGB 채널별 색상 이동: 다양한 카메라 환경, 화이트밸런스 차이 대응\n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.2),\n",
    "\n",
    "    # 이미지 일부분을 무작위로 지우기 (일부 가림 상황을 반영)\n",
    "    A.CoarseDropout(max_holes=2,          # 최대 2개 영역 제거\n",
    "                    max_height=16, \n",
    "                    max_width=16, \n",
    "                    p=0.3),\n",
    "\n",
    "    # 픽셀값을 정규화 (ImageNet 사전학습 모델 기준 평균/표준편차)\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    # numpy 이미지 → PyTorch Tensor 변환\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 이미지 수: 33137\n",
      "train 이미지 수: 26509, valid 이미지 수: 6628\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터셋 로드\n",
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"총 이미지 수: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "# Stratified Split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "# Subset + transform 각각 적용\n",
    "train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train 이미지 수: {len(train_dataset)}, valid 이미지 수: {len(val_dataset)}')\n",
    "\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name: str, num_classes: int):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)  # 분류기 제거\n",
    "\n",
    "        # 자동 in_features 추출\n",
    "        try:\n",
    "            in_features = self.backbone.num_features  # timm 공통 속성\n",
    "        except:\n",
    "            raise ValueError(f\"Could not find in_features for model {model_name}\")\n",
    "\n",
    "        # pooling 여부 결정 (ConvNeXt 같은 경우 필요)\n",
    "        self.needs_pooling = hasattr(self.backbone, 'head') and isinstance(self.backbone.head, nn.Identity) is False\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1)) if self._is_2d_output() else None\n",
    "\n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def _is_2d_output(self):\n",
    "        # ConvNeXt, EfficientNet 등은 (B, C, H, W)로 출력됨 → AdaptiveAvgPool 필요\n",
    "        example_input = torch.randn(1, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            output = self.backbone(example_input)\n",
    "        return output.dim() == 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        if self.pool:\n",
    "            x = self.pool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# resnet18, 1번과 2번\\nclass BaseModel(nn.Module):\\n    def __init__(self, num_classes):\\n        super(BaseModel, self).__init__()\\n        self.backbone = models.resnet18(pretrained=True)  # ResNet18 모델 불러오기\\n        self.feature_dim = self.backbone.fc.in_features \\n        self.backbone.fc = nn.Identity()  # feature extractor로만 사용\\n        self.head = nn.Linear(self.feature_dim, num_classes)  # 분류기\\n\\n    def forward(self, x):\\n        x = self.backbone(x)       \\n        x = self.head(x) \\n        return x\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# resnet18, 1번과 2번\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)  # ResNet18 모델 불러오기\n",
    "        self.feature_dim = self.backbone.fc.in_features \n",
    "        self.backbone.fc = nn.Identity()  # feature extractor로만 사용\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)  # 분류기\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)       \n",
    "        x = self.head(x) \n",
    "        return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\os415\\.cache\\huggingface\\hub\\models--timm--efficientnetv2_rw_s.ra2_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\os415\\AppData\\Local\\Temp\\ipykernel_5592\\2869195272.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "[Epoch 1/50] Training:   0%|          | 0/415 [00:00<?, ?it/s]C:\\Users\\os415\\AppData\\Local\\Temp\\ipykernel_5592\\2869195272.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # float16 연산\n",
      "[Epoch 1/50] Training:   5%|▌         | 22/415 [00:14<03:54,  1.67it/s]"
     ]
    }
   ],
   "source": [
    "model = BaseModel(model_name='ConvNeXt-Tiny', num_classes=len(class_names)).to(device)\n",
    "best_logloss = float('inf')\n",
    "# 추가: Early Stopping을 위한 변수\n",
    "patience = 5  # 개선 없을 때 몇 epoch까지 기다릴지\n",
    "counter = 0   # 현재까지 개선되지 않은 횟수\n",
    "\n",
    "# 손실 함수\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# 옵티마이저\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "\n",
    "# 스케일러\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 학습 및 검증 루프\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # LogLoss\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Best model 저장\n",
    "    if val_logloss < best_logloss:\n",
    "        best_logloss = val_logloss\n",
    "        torch.save(model.state_dict(), f'best_model.pth')\n",
    "        print(f\"📦 Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "        counter = 0  # 성능 개선되었으므로 초기화\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"⚠️ No improvement for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            print(f\"⏹ Early stopping triggered at epoch {epoch+1}\")\n",
    "            break  # 학습 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset(test_root, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 로드\n",
    "model = BaseModel(model_name='ConvNeXt-Tiny', num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # 각 배치의 확률을 리스트로 변환\n",
    "        for prob in probs.cpu():  # prob: (num_classes,)\n",
    "            result = {\n",
    "                class_names[i]: prob[i].item()\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "pred = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./result/sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 'ID' 컬럼을 제외한 클래스 컬럼 정렬\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('./result/submission_04.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
