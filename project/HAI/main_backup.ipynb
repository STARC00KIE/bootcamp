{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수정 포인트\n",
    "1. 데이터 증강 추가, 기존에는 정규화만 있었음, resnet18 백본 사용\n",
    "2. Early Stopping, resnet18 백본 사용\n",
    "3. ConvNeXt-Tiny로 백본 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'IMG_SIZE': 224, # 모델에 입력되는 이미지 크기\n",
    "# 'BATCH_SIZE': 64, # 한 번에 모델에 넣는 이미지 수\n",
    "# 'EPOCHS': 50, # 전체 데이터를 몇 번 반복 학습할지\n",
    "#'LEARNING_RATE': 가중치 업데이트 정도를 조절하는 학습률\n",
    "# 'SEED' : 무작위 요소들을 고정하여 실험 결과를 재현 가능하게 함\n",
    "# \n",
    "CFG = {\n",
    "    'IMG_SIZE': 224,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'EPOCHS': 50,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 학습 과정에서는 무작위성이 많이 개입된\n",
    "# 무작위성: 데이터 로딩 순서, 가중치 초기화, dropout, 데이터 증강, gpu 연산의 비결정성\n",
    "# 그레서 시드를 고정하지 안으면 매번 실행할 때마다 결과가 달라짐\n",
    "# 동일한 결과가 재현되도록 하기 위한 설정이 seed_everyting 함수수\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            # 테스트셋: 라벨 없이 이미지 경로만 저장\n",
    "            for fname in sorted(os.listdir(root_dir)):\n",
    "                if fname.lower().endswith(('.jpg')):\n",
    "                    img_path = os.path.join(root_dir, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            # 학습셋: 클래스별 폴더 구조에서 라벨 추출\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(root_dir, cls_name)\n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith(('.jpg')):\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = './data/train'\n",
    "test_root = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\albumentations\\core\\validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\os415\\AppData\\Local\\Temp\\ipykernel_29460\\2295189183.py:28: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=2,          # 최대 2개 영역 제거\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터에 적용할 데이터 증강 파이프라인\n",
    "train_transform = A.Compose([\n",
    "    # 이미지 크기를 모델 입력 크기에 맞게 고정\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "\n",
    "    # 좌우 반전: 차량이 좌우 어느 방향을 향해 있어도 학습 가능하도록\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "\n",
    "    # 밝기/대비 랜덤 조절: 야외 촬영 시 조명 차이를 반영\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "\n",
    "    # 색조(Hue), 채도(Saturation), 명도(Value) 변경: 색상에 덜 민감하게\n",
    "    A.HueSaturationValue(p=0.2),\n",
    "\n",
    "    # 이미지 이동, 확대/축소, 회전: 다양한 촬영 각도 및 위치 대응\n",
    "    A.ShiftScaleRotate(shift_limit=0.05,   # 최대 ±5% 이동\n",
    "                       scale_limit=0.05,   # 최대 ±5% 확대/축소\n",
    "                       rotate_limit=15,    # 최대 ±15도 회전\n",
    "                       p=0.5),\n",
    "\n",
    "    # 그림자 효과 추가: 자연광 환경에서의 촬영 상황을 반영\n",
    "    A.RandomShadow(p=0.2),\n",
    "\n",
    "    # RGB 채널별 색상 이동: 다양한 카메라 환경, 화이트밸런스 차이 대응\n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.2),\n",
    "\n",
    "    # 이미지 일부분을 무작위로 지우기 (일부 가림 상황을 반영)\n",
    "    A.CoarseDropout(max_holes=2,          # 최대 2개 영역 제거\n",
    "                    max_height=16, \n",
    "                    max_width=16, \n",
    "                    p=0.3),\n",
    "\n",
    "    # 픽셀값을 정규화 (ImageNet 사전학습 모델 기준 평균/표준편차)\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    # numpy 이미지 → PyTorch Tensor 변환\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 이미지 수: 33137\n",
      "train 이미지 수: 26509, valid 이미지 수: 6628\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터셋 로드\n",
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"총 이미지 수: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "# Stratified Split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "# Subset + transform 각각 적용\n",
    "train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train 이미지 수: {len(train_dataset)}, valid 이미지 수: {len(val_dataset)}')\n",
    "\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name: str, num_classes: int):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)  # 분류기 제거\n",
    "\n",
    "        # 자동 in_features 추출\n",
    "        try:\n",
    "            in_features = self.backbone.num_features  # timm 공통 속성\n",
    "        except:\n",
    "            raise ValueError(f\"Could not find in_features for model {model_name}\")\n",
    "\n",
    "        # pooling 여부 결정 (ConvNeXt 같은 경우 필요)\n",
    "        self.needs_pooling = hasattr(self.backbone, 'head') and isinstance(self.backbone.head, nn.Identity) is False\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1)) if self._is_2d_output() else None\n",
    "\n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def _is_2d_output(self):\n",
    "        # ConvNeXt, EfficientNet 등은 (B, C, H, W)로 출력됨 → AdaptiveAvgPool 필요\n",
    "        example_input = torch.randn(1, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            output = self.backbone(example_input)\n",
    "        return output.dim() == 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        if self.pool:\n",
    "            x = self.pool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# resnet18, 1번과 2번\\nclass BaseModel(nn.Module):\\n    def __init__(self, num_classes):\\n        super(BaseModel, self).__init__()\\n        self.backbone = models.resnet18(pretrained=True)  # ResNet18 모델 불러오기\\n        self.feature_dim = self.backbone.fc.in_features \\n        self.backbone.fc = nn.Identity()  # feature extractor로만 사용\\n        self.head = nn.Linear(self.feature_dim, num_classes)  # 분류기\\n\\n    def forward(self, x):\\n        x = self.backbone(x)       \\n        x = self.head(x) \\n        return x\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# resnet18, 1번과 2번\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)  # ResNet18 모델 불러오기\n",
    "        self.feature_dim = self.backbone.fc.in_features \n",
    "        self.backbone.fc = nn.Identity()  # feature extractor로만 사용\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)  # 분류기\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)       \n",
    "        x = self.head(x) \n",
    "        return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Training: 100%|██████████| 415/415 [10:23<00:00,  1.50s/it]\n",
      "[Epoch 1/50] Validation: 100%|██████████| 104/104 [00:50<00:00,  2.07it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 4.5783 || Valid Loss : 0.9014 | Valid Accuracy : 75.9656%\n",
      "📦 Best model saved at epoch 1 (logloss: 0.9019)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2/50] Training: 100%|██████████| 415/415 [10:04<00:00,  1.46s/it]\n",
      "[Epoch 2/50] Validation: 100%|██████████| 104/104 [00:45<00:00,  2.30it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.4583 || Valid Loss : 0.2911 | Valid Accuracy : 90.8570%\n",
      "📦 Best model saved at epoch 2 (logloss: 0.2916)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3/50] Training: 100%|██████████| 415/415 [10:03<00:00,  1.45s/it]\n",
      "[Epoch 3/50] Validation: 100%|██████████| 104/104 [00:45<00:00,  2.30it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.1899 || Valid Loss : 0.2283 | Valid Accuracy : 93.1955%\n",
      "📦 Best model saved at epoch 3 (logloss: 0.2287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4/50] Training: 100%|██████████| 415/415 [10:05<00:00,  1.46s/it]\n",
      "[Epoch 4/50] Validation: 100%|██████████| 104/104 [00:46<00:00,  2.22it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.1278 || Valid Loss : 0.1958 | Valid Accuracy : 94.0555%\n",
      "📦 Best model saved at epoch 4 (logloss: 0.1962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5/50] Training: 100%|██████████| 415/415 [09:48<00:00,  1.42s/it]\n",
      "[Epoch 5/50] Validation: 100%|██████████| 104/104 [00:43<00:00,  2.40it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.1060 || Valid Loss : 0.2154 | Valid Accuracy : 93.4671%\n",
      "⚠️ No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6/50] Training: 100%|██████████| 415/415 [09:50<00:00,  1.42s/it]\n",
      "[Epoch 6/50] Validation: 100%|██████████| 104/104 [00:44<00:00,  2.36it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0970 || Valid Loss : 0.1991 | Valid Accuracy : 94.5987%\n",
      "⚠️ No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 7/50] Training: 100%|██████████| 415/415 [09:34<00:00,  1.38s/it]\n",
      "[Epoch 7/50] Validation: 100%|██████████| 104/104 [00:43<00:00,  2.38it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0924 || Valid Loss : 0.2105 | Valid Accuracy : 93.8745%\n",
      "⚠️ No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 8/50] Training: 100%|██████████| 415/415 [09:48<00:00,  1.42s/it]\n",
      "[Epoch 8/50] Validation: 100%|██████████| 104/104 [00:45<00:00,  2.28it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0796 || Valid Loss : 0.1926 | Valid Accuracy : 94.6741%\n",
      "📦 Best model saved at epoch 8 (logloss: 0.1927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 9/50] Training: 100%|██████████| 415/415 [14:32<00:00,  2.10s/it]\n",
      "[Epoch 9/50] Validation: 100%|██████████| 104/104 [01:07<00:00,  1.55it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0764 || Valid Loss : 0.1720 | Valid Accuracy : 95.2173%\n",
      "📦 Best model saved at epoch 9 (logloss: 0.1725)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 10/50] Training: 100%|██████████| 415/415 [13:41<00:00,  1.98s/it]\n",
      "[Epoch 10/50] Validation: 100%|██████████| 104/104 [01:05<00:00,  1.59it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0698 || Valid Loss : 0.1802 | Valid Accuracy : 95.0966%\n",
      "⚠️ No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 11/50] Training: 100%|██████████| 415/415 [13:14<00:00,  1.91s/it]\n",
      "[Epoch 11/50] Validation: 100%|██████████| 104/104 [01:02<00:00,  1.65it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0705 || Valid Loss : 0.2142 | Valid Accuracy : 94.4176%\n",
      "⚠️ No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 12/50] Training: 100%|██████████| 415/415 [14:55<00:00,  2.16s/it]\n",
      "[Epoch 12/50] Validation: 100%|██████████| 104/104 [01:18<00:00,  1.32it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0707 || Valid Loss : 0.2065 | Valid Accuracy : 94.3874%\n",
      "⚠️ No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 13/50] Training: 100%|██████████| 415/415 [14:00<00:00,  2.03s/it]\n",
      "[Epoch 13/50] Validation: 100%|██████████| 104/104 [01:11<00:00,  1.45it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0631 || Valid Loss : 0.1691 | Valid Accuracy : 95.7302%\n",
      "📦 Best model saved at epoch 13 (logloss: 0.1697)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 14/50] Training: 100%|██████████| 415/415 [14:01<00:00,  2.03s/it]\n",
      "[Epoch 14/50] Validation: 100%|██████████| 104/104 [01:11<00:00,  1.46it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0492 || Valid Loss : 0.1576 | Valid Accuracy : 95.8358%\n",
      "📦 Best model saved at epoch 14 (logloss: 0.1581)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 15/50] Training: 100%|██████████| 415/415 [13:33<00:00,  1.96s/it]\n",
      "[Epoch 15/50] Validation: 100%|██████████| 104/104 [01:07<00:00,  1.55it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0427 || Valid Loss : 0.2042 | Valid Accuracy : 95.2323%\n",
      "⚠️ No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 16/50] Training: 100%|██████████| 415/415 [13:37<00:00,  1.97s/it]\n",
      "[Epoch 16/50] Validation: 100%|██████████| 104/104 [01:02<00:00,  1.66it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0543 || Valid Loss : 0.1676 | Valid Accuracy : 95.5944%\n",
      "⚠️ No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 17/50] Training: 100%|██████████| 415/415 [13:16<00:00,  1.92s/it]\n",
      "[Epoch 17/50] Validation: 100%|██████████| 104/104 [00:57<00:00,  1.80it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0446 || Valid Loss : 0.1839 | Valid Accuracy : 95.8057%\n",
      "⚠️ No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 18/50] Training: 100%|██████████| 415/415 [14:02<00:00,  2.03s/it]\n",
      "[Epoch 18/50] Validation: 100%|██████████| 104/104 [01:06<00:00,  1.56it/s]\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0553 || Valid Loss : 0.1772 | Valid Accuracy : 95.6548%\n",
      "⚠️ No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 19/50] Training: 100%|██████████| 415/415 [12:48<00:00,  1.85s/it]\n",
      "[Epoch 19/50] Validation: 100%|██████████| 104/104 [01:02<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0484 || Valid Loss : 0.1704 | Valid Accuracy : 95.7453%\n",
      "⚠️ No improvement for 5 epoch(s).\n",
      "⏹ Early stopping triggered at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\os415\\.conda\\envs\\hai\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(model_name='efficientnetv2_rw_s', num_classes=len(class_names)).to(device)\n",
    "best_logloss = float('inf')\n",
    "# 추가: Early Stopping을 위한 변수\n",
    "patience = 5  # 개선 없을 때 몇 epoch까지 기다릴지\n",
    "counter = 0   # 현재까지 개선되지 않은 횟수\n",
    "\n",
    "# 손실 함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "\n",
    "# 학습 및 검증 루프\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # LogLoss\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Best model 저장\n",
    "    if val_logloss < best_logloss:\n",
    "        best_logloss = val_logloss\n",
    "        torch.save(model.state_dict(), f'best_model.pth')\n",
    "        print(f\"📦 Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "        counter = 0  # 성능 개선되었으므로 초기화\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"⚠️ No improvement for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            print(f\"⏹ Early stopping triggered at epoch {epoch+1}\")\n",
    "            break  # 학습 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset(test_root, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 로드\n",
    "model = BaseModel(model_name='efficientnetv2_rw_s', num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # 각 배치의 확률을 리스트로 변환\n",
    "        for prob in probs.cpu():  # prob: (num_classes,)\n",
    "            result = {\n",
    "                class_names[i]: prob[i].item()\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "pred = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./result/sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 'ID' 컬럼을 제외한 클래스 컬럼 정렬\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('./result/submission_04.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
